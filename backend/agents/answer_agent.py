from dotenv import load_dotenv
import os

from langchain_community.chat_models import ValyuChat
from langchain_core.messages import SystemMessage, HumanMessage

from backend.core.models import ContextSelectionResult, ScoredChunk

load_dotenv()


def get_llm():
    """Create the Valyu LLM client using team credentials."""
    return ValyuChat(
        api_key=os.getenv("VALYU_API_TOKEN"),
        team_id=os.getenv("VALYU_TEAM_ID"),
        model=os.getenv("VALYU_MODEL"),
        endpoint=os.getenv("VALYU_API_ENDPOINT"),
        max_retries=2,
    )


class AnswerAgent:
    """
    Generates a final response using the selected context.

    Input:
        ContextSelectionResult
            Contains the user query and a list of selected ScoredChunk objects.

    Output:
        dict with:
            "final_output" : the text generated by the LLM
            "full_prompt"  : the combined system and user prompt (for transparency)
    """

    def __init__(self):
        self.llm = get_llm()

    @staticmethod
    def _format_chunks(chunks: list[ScoredChunk]) -> str:
        """
        Format selected chunks into a consistent textual block that the LLM can read.
        """
        blocks = []
        for scored in chunks:
            c = scored.chunk
            header = f"[{c.chunk_id}] from {c.file_path} (lines {c.start_line}-{c.end_line})"
            blocks.append(f"{header}\n{c.text.strip()}\n")
        return "\n".join(blocks)

    def run(self, context: ContextSelectionResult) -> dict:
        """
        Produce the final answer using the LLM.
        """
        chunk_text = self._format_chunks(context.selected_chunks)

        system_prompt = (
            "You are a precise and context-grounded assistant. "
            "Use only the provided context chunks when generating the answer."
        )

        user_prompt = (
            f"User request:\n{context.query}\n\n"
            f"Relevant context chunks:\n{chunk_text}\n"
            "Instructions:\n"
            "1. Only use information present in the chunks.\n"
            "2. Generate the exact output needed for the request.\n"
            "3. Do not reveal chain of thought.\n"
        )

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ]

        response = self.llm.invoke(messages)

        return {
            "final_output": response.content.strip(),
            "full_prompt": system_prompt + "\n\n" + user_prompt,
        }
